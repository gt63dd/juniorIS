% Here is an example of how to create a bibliography entry for an article using
% BibTeX. Generally you won't have to write these out yourself, because they are
% provided by most web sites that allow you to export citations. The string
% "clrsAlgorithms" is a citation key, and if you were citing the source in a
% document you would use \cite{clrsAlgorithms}.

}

@article{rahman2020new,
author = {Rahman, Rizwan Ur and Tomar, Deepak Singh},
title = {New biostatistics features for detecting web bot activity on web applications},
journal = {Computers \& Security},
volume = {97},
pages = {102001},
year = {2020},
publisher = {Elsevier}
}

@article{rahman2020forensic,
author = {Rahman, Rizwan Ur and Tomar, Deepak Singh},
title = {A new web forensic framework for bot crime investigation},
journal = {Forensic Science International: Digital Investigation},
volume = {33},
pages = {300943},
year = {2020},
publisher = {Elsevier}
}

@article{tanaka2020bot,
author = {Tanaka, Takamasa and Niibori, Hidekazu and Nomura, Shimpei and Kawashima, Hiroki and Tsuda, Kazuhiko and others},
title = {Bot detection model using user agent and user behavior for web log analysis},
journal = {Procedia Computer Science},
volume = {176},
pages = {1621--1625},
year = {2020},
publisher = {Elsevier},
annote = {This paper focuses on detecting bots in web traffic when bots can spoof user agent strings. The authors argue that relying on user agent alone is unreliable, so they combine user agent information with user behavior features extracted from web logs. They define visits as cookie-based sessions with an inactivity timeout and propose a detection model that improves bot identification by incorporating patterns of navigation and request behavior. A major practical contribution is the discussion of labeling: they suggest using whether JavaScript executed as a signal for human activity, while acknowledging that labeling can be imperfect due to network conditions and because advanced bots may execute JavaScript. The paper frames bot filtering as essential for preventing bots from corrupting analytics and decision-making systems built on web log data. Credibility-wise, it is published as a conference proceedings style paper, so it is shorter and less detailed than a full journal study, but it still presents a clear and realistic approach grounded in production-style data. For my reseller bot project, this directly supports two design choices: treat user agent as a weak feature, and prioritize behavioral plus client capability signals to catch automation that tries to blend in.}
}

@article{stassopoulou2009web,
author = {Stassopoulou, Athena and Dikaiakos, Marios D},
title = {Web robot detection: A probabilistic reasoning approach},
journal = {Computer Networks},
volume = {53},
number = {3},
pages = {265--278},
year = {2009},
publisher = {Elsevier},
annote = {This article proposes a web robot detection approach that relies on server access logs and session-level classification. The authors first transform raw log requests into browsing sessions using rules that split visits based on inactivity thresholds, then compute behavioral features from each session. They classify sessions as human or robot using a Bayesian network, which is useful because it produces not only a label but also a probability score that reflects confidence. The paper emphasizes that single indicators (like user agent strings) are weak on their own, so combining multiple signals improves reliability. The evaluation uses real log data and reports strong detection performance, while also acknowledging failure cases such as short sessions, ambiguous browsing patterns, or traffic types that do not fit the training distribution. From a credibility standpoint, this is a peer reviewed journal article with a clear, reproducible modeling pipeline and a realistic data source. The main limitation is that it is log-centric, so it may miss modern client-side indicators (JavaScript execution, mouse movement, typing cadence). For my reseller bot project, this paper supports building a baseline risk scoring model from session features, then using the score to trigger mitigation like throttling, step-up checks, or queue placement.}
}

  
@inbook{10.1145/3672608.3707741,
author = {Gangwal, Ankit and Reddy, P. Sahithi and Sagar, C. y. k.},
title = {Swiss Cheese CAPTCHA: A Novel Multi-barrier Mechanism for Bot Detection},
year = {2025},
isbn = {9798400706295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672608.3707741},
abstract = {A CAPTCHA is one of the primary barriers between notorious bots and legitimate human users. However, advancements in Artificial Intelligence (AI) have enabled malicious bots to circumvent CAPTCHA challenges effectively. As a result, several types of CAPTCHA have been rendered ineffective. In this paper, we introduce Swiss Cheese CAPTCHA, a novel sensor-based solution designed to be easily solvable by humans while presenting multiple obstructions for bots (similar to the Swiss Cheese Model) even when the sensor outputs can be predicted and interfered with. We leverage a range of human cognitive abilities and Generic Sensor API in modern devices to provide robust protection against automated attacks by making it more computationally expensive for bots to produce a valid answer within a stipulated time. We conducted two user studies to assess our proposal's effectiveness: one involving 116 participants to assess the likability and improvise the design, and the other, with 107 participants, to investigate the impact of improvised design changes on cognitive abilities. Our results from these studies show an average completion time of 4.76 seconds and 6.12 seconds, with a success rate of 90.3\% and 83.25\%, respectively. By analyzing the 2141 resultant trajectories from both user studies, we assess the learnability, error recovery rate, efficiency, and satisfaction of users using the scheme. Finally, we devise an automated attack against our proposal to analyze its security in the real world; we find the probability of attack success is low. We also make our research artifacts publicly available.},
booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing},
pages = {1780--1789},
numpages = {10},
annote = {The paper proposes "Swiss Cheese CAPTCHA," a multi-step challenge meant to stay easy for humans but harder for bots by stacking several independent obstacles under a time limit, including signals from device sensors through the Generic Sensor API. It reports two user studies showing most people can complete the challenge in about 5–6 seconds with high success rates, and it uses trajectory data to study how users recover from mistakes and learn the task. The paper also tests security by building an automated attack and finds a low success rate for that attack in their experiments. A strong point is that it evaluates both usability and security, which matters for real e-commerce defenses, while a limitation is that the sensor-based design may not work as well in desktop-heavy reseller scenarios where sensor access is limited. For a navigation-pattern risk scoring system, the main value is as an escalation tool: suspicious sessions can be routed into a stronger challenge instead of being blocked outright, and the paper offers a clear way to measure the security–friction tradeoff when adding that step to a layered defense pipeline.}




@article{10.1145/3447815,
author = {Iliou, Christos and Kostoulas, Theodoros and Tsikrika, Theodora and Katos, Vasilis and Vrochidis, Stefanos and Kompatsiaris, Ioannis},
title = {Detection of Advanced Web Bots by Combining Web Logs with Mouse Behavioural Biometrics},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3447815},
doi = {10.1145/3447815},
abstract = {Web bots vary in sophistication based on their purpose, ranging from simple automated scripts to advanced web bots that have a browser fingerprint, support the main browser functionalities, and exhibit a humanlike behaviour. Advanced web bots are especially appealing to malicious web bot creators, due to their browserlike fingerprint and humanlike behaviour that reduce their detectability. This work proposes a web bot detection framework that comprises two detection modules: (i) a detection module that utilises web logs, and (ii) a detection module that leverages mouse movements. The framework combines the results of each module in a novel way to capture the different temporal characteristics of the web logs and the mouse movements, as well as the spatial characteristics of the mouse movements. We assess its effectiveness on web bots of two levels of evasiveness: (a) moderate web bots that have a browser fingerprint and (b) advanced web bots that have a browser fingerprint and also exhibit a humanlike behaviour. We show that combining web logs with visitors’ mouse movements is more effective and robust toward detecting advanced web bots that try to evade detection, as opposed to using only one of those approaches.},
journal = {Digital Threats},
month = jun,
articleno = {24},
numpages = {26},
keywords = {mouse movements, mouse biometrics, humanlike behaviour, evasive web bots, advanced web bots, Web bot detection}
,
annote = {This paper presents a detection framework aimed at advanced web bots that spoof browser fingerprints and attempt to imitate human browsing. It builds two separate detectors: one based on session-level web-log features (navigation paths, timing, and request patterns) and another based on mouse-movement features, then fuses the two scores with a rule that relies more heavily on mouse evidence and uses the web-log score when the case is uncertain. The evaluation uses a controlled test site with human sessions plus two bot classes, moderate bots that mainly spoof fingerprints and advanced bots that also simulate humanlike browsing and mouse behavior. Results show the fused approach performs better than a web-log-only detector and remains effective under attempted behavioral imitation, with very strong reported classification metrics. A key limitation is external validity: because the study uses a test environment and simulated bots, real reseller-bot campaigns that use proxy rotation, distributed accounts, and trace replay could shift the feature distributions and reduce performance.}
}




@inproceedings{10.1145/3538969.3538994,
author = {Iliou, Christos and Kostoulas, Theodoros and Tsikrika, Theodora and Katos, Vasilios and Vrochidis, Stefanos and Kompatsiaris, Ioannis},
title = {Web Bot Detection Evasion Using Deep Reinforcement Learning},
year = {2022},
isbn = {9781450396707},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538969.3538994},
doi = {10.1145/3538969.3538994},
abstract = {Web bots are vital for the web as they can be used to automate several actions, some of which would have otherwise been impossible or very time consuming. These actions can be benign, such as website testing and web indexing, or malicious, such as unauthorised content scraping, scalping, vulnerability scanning, and more. To detect malicious web bots, recent approaches examine the visitors’ fingerprint and behaviour. For the latter, several values (i.e., features) are usually extracted from visitors’ web logs and used as input to train machine learning models. In this research we show that web bots can use recent advances in machine learning, and, more specifically, Reinforcement Learning (RL), to effectively evade behaviour-based detection techniques. To evaluate these evasive bots, we examine (i) how well they can evade a pre-trained bot detection framework, (ii) how well they can still evade detection after the detection framework is re-trained on new behaviours generated from the evasive web bots, and (iii) how bots perform if re-trained again on the re-trained detection framework. We show that web bots can repeatedly evade detection and adapt to the re-trained detection framework to showcase the importance of considering such types of bots when designing web bot detection frameworks.},
booktitle = {Proceedings of the 17th International Conference on Availability, Reliability and Security},
articleno = {15},
numpages = {10},
keywords = {advanced web bots, evasive web bots, reinforcement learning, web bot detection, web logs},
location = {Vienna, Austria},
series = {ARES '22}
,
annote = {The paper examines whether bots can learn browsing and navigation patterns that slip past behavior-based detectors built from web-log features. It models evasion as a reinforcement learning problem where a bot chooses actions to shape session features while still completing its goal, using a pre-trained detector as the environment and getting rewarded when it is not flagged. Results show simple scripted bots rarely evade, but RL-trained bots can learn evasive strategies and reach much higher evasion rates, with some settings approaching about half of bot sessions avoiding detection after enough training. The paper then shows an attacker–defender loop: once the detector is retrained on the new behaviors, bots can train again and regain evasion capability. A key strength is the explicit adversarial setup and staged evaluation over time, while a limitation is that the experiments focus mainly on manipulating web-log features and do not fully incorporate real reseller-bot constraints like identity and payment linkages, queueing, and rate limits. For navigation-pattern risk scoring, the paper provides a clear threat model and a reusable stress-test method by training adaptive bots to minimize a risk score while still reaching add-to-cart and checkout.}
}







